# Text Summarization Using Transformer Encoder from Scratch

## Author: Ahmed Essam Abd Elgwad

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)  
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)  
![License](https://img.shields.io/badge/License-MIT-green)  

A **PyTorch-based Transformer Encoder model** for **extractive text summarization**. This project focuses on identifying and extracting the most important sentences from a given text.

---

## ğŸ“Œ Table of Contents
1. [Overview](#overview)
2. [Features](#features)
3. [Installation](#installation)
4. [Usage](#usage)
5. [Results](#results)
6. [Project Structure](#project-structure)
7. [Contributing](#contributing)
8. [License](#license)
9. [Acknowledgements](#acknowledgements)
10. [Contact](#contact)

---

## ğŸ” Overview
Text summarization is the process of condensing a large body of text into a concise version while retaining key information. This project implements a **Transformer Encoder-based model** to perform **extractive summarization**, where sentences are classified as **important (1)** or **not important (0)** for inclusion in the summary.

### âœ¨ Key Components
- **Transformer Encoder** â€“ Captures contextual relationships between words.
- **Mean Pooling** â€“ Aggregates token-level outputs into a sentence-level representation.
- **Binary Classification** â€“ Determines the importance of each sentence.
- **PyTorch Implementation** â€“ Built from scratch for transparency and flexibility.

---

## ğŸš€ Features
âœ… **Custom Transformer Encoder** â€“ No external pre-trained models used.  
âœ… **Extractive Summarization** â€“ Selects key sentences instead of generating new text.  
âœ… **Modular Design** â€“ Clean, reusable, and extendable code structure.  
âœ… **GPU Support** â€“ Runs efficiently on CUDA-enabled GPUs.  
âœ… **Easy Integration** â€“ Plug-and-play architecture for further improvements.  

---

## ğŸ›  Installation
1. **Clone the Repository**:
   ```bash
   git clone https://github.com/your-username/text-summarization-project.git
   cd text-summarization-project
   ```
2. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ“Œ Usage
### ğŸ”¹ Training the Model
To train the model, run:
```bash
python train.py
```

### ğŸ”¹ Evaluating the Model
To evaluate the model, run:
```bash
python evaluate.py
```

### ğŸ”¹ Customizing the Input
- Place your text file inside the `data/` directory.
- Update the `file_path` variable in `train.py` and `evaluate.py` to point to your input file.


---

## ğŸ“ Project Structure
```plaintext
text_summarization_project/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ text1.txt                 # Sample input text file
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py                # Package initializer
â”‚   â”œâ”€â”€ transformer_encoder.py     # Transformer Encoder implementation
â”‚   â””â”€â”€ summarization_model.py     # Summarization model script
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py                # Package initializer
â”‚   â”œâ”€â”€ preprocessing.py           # Text preprocessing functions
â”‚   â””â”€â”€ evaluation.py              # Evaluation functions
â”‚
â”œâ”€â”€ train.py                        # Model training script
â”œâ”€â”€ evaluate.py                     # Model evaluation script
â”œâ”€â”€ requirements.txt                # List of dependencies
â”œâ”€â”€ README.md                       # Project documentation
â””â”€â”€ LICENSE                         # License details
```

---

## ğŸ¤ Contributing
Contributions are welcome! To contribute:
1. **Fork the repository.**
2. **Create a new branch:**  
   ```bash
   git checkout -b feature/YourFeature
   ```
3. **Commit your changes:**  
   ```bash
   git commit -m "Add new feature"
   ```
4. **Push to the branch:**  
   ```bash
   git push origin feature/YourFeature
   ```
5. **Open a Pull Request.**

---

## ğŸ“œ License
This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## ğŸ™Œ Acknowledgements
- **[PyTorch](https://pytorch.org/)** â€“ Deep learning framework used for model implementation.
- **[Attention is All You Need](https://arxiv.org/abs/1706.03762)** â€“ Transformer model architecture paper.
- **Ahmed Essam** â€“ Project development and documentation.

---

## ğŸ“§ Contact
For questions or feedback, feel free to reach out:
ğŸ“© **Email:** ahmedessam2996@gamil.com  
ğŸ”— **GitHub:** [Ahmed Essam](https://github.com/AhmedEssam29)  

---

ğŸ“Œ **If you find this project helpful, don't forget to â­ the repository!** ğŸš€

